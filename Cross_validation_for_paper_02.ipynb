{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43b01b-6261-4139-be7b-80adae34226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efa02a-053a-43b9-b640-e423459f1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adult = pd.read_csv(r\"/content/drive/MyDrive/Datasets_for_paper_02/uciml_data.csv\")\n",
    "df_adult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cbb52-7201-457e-a742-92d528a291de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compas = pd.read_csv(r\"/content/drive/MyDrive/Datasets_for_paper_02/compas_df.csv\")\n",
    "df_compas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3c5c5-1da4-4d81-8af9-386fb4ef2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_german = pd.read_csv(r\"/content/drive/MyDrive/Datasets_for_paper_02/german_df.csv\")\n",
    "df_german.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da074a9-2108-4635-a2b9-6d6812277cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Imports ────────────────────────────────────────────────────────────────────\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── DataFrames are already loaded as df_adult, df_compas, df_german ────────────\n",
    "# This code will use your pre-loaded DataFrames.\n",
    "\n",
    "# ─── Feature Engineering & Definition (Based on YOUR column names) ──────────────\n",
    "\n",
    "# Adult Dataset\n",
    "df_adult['label'] = (df_adult['income'].str.strip() == '>50K').astype(int)\n",
    "df_adult['gender_numeric'] = (df_adult['gender'].str.strip() == 'Male').astype(int)\n",
    "df_adult['race_numeric'] = (df_adult['race'].str.strip() == 'White').astype(int)\n",
    "\n",
    "# COMPAS Dataset\n",
    "df_compas['label'] = df_compas['two_year_recid']\n",
    "df_compas['gender_numeric'] = (df_compas['sex'].str.strip() == 'Male').astype(int)\n",
    "df_compas['race_numeric'] = (df_compas['race'].str.strip() == 'Caucasian').astype(int)\n",
    "\n",
    "# German Credit Dataset\n",
    "df_german['label'] = (df_german['target'] == 2).astype(int)\n",
    "df_german['gender_numeric'] = df_german['personal_status'].map(lambda x: 1 if str(x).strip() in ['A91','A93', 'A94'] else 0)\n",
    "\n",
    "\n",
    "# --- THE DEFINITIVE FIX: A strict \"allow-list\" of known safe features based on YOUR columns ---\n",
    "RAW_FEATURES = {\n",
    "    'adult': [\n",
    "        'age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
    "        'marital-status', 'occupation', 'relationship', 'capital-gain',\n",
    "        'capital-loss', 'hours-per-week', 'native-country'\n",
    "    ],\n",
    "    'compas': [\n",
    "        # Features used in the original ProPublica analysis, excluding leaky ones\n",
    "        'age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count',\n",
    "        'priors_count', 'c_charge_degree'\n",
    "    ],\n",
    "    'german': [\n",
    "        'status', 'duration', 'credit_history', 'purpose', 'amount', 'savings',\n",
    "        'employment', 'installment_rate', 'other_debtors',\n",
    "        'residence_since', 'property', 'age', 'other_installment_plans',\n",
    "        'housing', 'existing_credits', 'job', 'num_people_liable', 'telephone',\n",
    "        'foreign_worker'\n",
    "    ]\n",
    "}\n",
    "\n",
    "datasets = {'adult': df_adult, 'compas': df_compas, 'german': df_german}\n",
    "prot_map = {'adult': 'gender_numeric', 'compas': 'race_numeric', 'german': 'gender_numeric'}\n",
    "\n",
    "# ─── Fairness metrics ───────────────────────────────────────────────────────────\n",
    "def compute_spd(y_pred, sens): return np.mean(y_pred[sens == 1]) - np.mean(y_pred[sens == 0])\n",
    "def compute_eod(y, y_pred, sens):\n",
    "    y, y_pred, sens = np.asarray(y), np.asarray(y_pred), np.asarray(sens)\n",
    "    tprs = [((y[(sens==g)]==1) & (y_pred[(sens==g)]==1)).sum()/max(1,(y[(sens==g)]==1).sum()) for g in [0,1]]\n",
    "    return tprs[1] - tprs[0]\n",
    "\n",
    "# ─── Cross-Validation Loop ──────────────────────────────────────────────────────\n",
    "results = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "methods = ['baseline', 'pre', 'in', 'post']\n",
    "\n",
    "for name, df in tqdm(datasets.items(), desc=\"Processing datasets\"):\n",
    "    prot_col_name = prot_map[name]\n",
    "    \n",
    "    # --- Create X, y, s from the safe lists, ensuring no leakage ---\n",
    "    X = df[RAW_FEATURES[name]].copy()\n",
    "    y = df['label'].copy()\n",
    "    s = df[prot_col_name].copy()\n",
    "    \n",
    "    stratify_key = y.astype(str) + '_' + s.astype(str)\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    prep = ColumnTransformer([\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ])\n",
    "\n",
    "    for method in tqdm(methods, desc=f\"Methods for {name}\", leave=False):\n",
    "        for train_idx, test_idx in cv.split(X, stratify_key):\n",
    "            X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
    "            X_te, y_te = X.iloc[test_idx], y.iloc[test_idx]\n",
    "            s_tr, s_te = s.iloc[train_idx].values, s.iloc[test_idx].values\n",
    "            \n",
    "            model = HistGradientBoostingClassifier(random_state=42)\n",
    "            X_tr_proc = prep.fit_transform(X_tr)\n",
    "            X_te_proc = prep.transform(X_te)\n",
    "            \n",
    "            sw, y_pred = None, None\n",
    "\n",
    "            if method == 'pre':\n",
    "                df_tr_proc = pd.DataFrame(X_tr_proc, columns=prep.get_feature_names_out())\n",
    "                df_tr_proc['label'] = y_tr.values\n",
    "                df_tr_proc[prot_col_name] = s_tr\n",
    "                sd = StandardDataset(df_tr_proc, label_name='label', favorable_classes=[1],\n",
    "                                     protected_attribute_names=[prot_col_name], privileged_classes=[[1]])\n",
    "                rw = Reweighing(unprivileged_groups=[{prot_col_name: 0}], privileged_groups=[{prot_col_name: 1}])\n",
    "                sd_transformed = rw.fit_transform(sd)\n",
    "                sw, y_tr = sd_transformed.instance_weights, sd_transformed.labels.ravel()\n",
    "\n",
    "            try:\n",
    "                if method in ['baseline', 'pre']:\n",
    "                    model.fit(X_tr_proc, y_tr, sample_weight=sw)\n",
    "                    y_pred = model.predict(X_te_proc)\n",
    "                elif method == 'post':\n",
    "                    model.fit(X_tr_proc, y_tr)\n",
    "                    post_opt = ThresholdOptimizer(estimator=model, constraints=\"equalized_odds\", prefit=True)\n",
    "                    post_opt.fit(X_tr_proc, y_tr, sensitive_features=s_tr)\n",
    "                    y_pred = post_opt.predict(X_te_proc, sensitive_features=s_te)\n",
    "                else: # 'in'\n",
    "                    mitigator = ExponentiatedGradient(estimator=model, constraints=EqualizedOdds())\n",
    "                    mitigator.fit(X_tr_proc, y_tr, sensitive_features=s_tr)\n",
    "                    y_pred = mitigator.predict(X_te_proc)\n",
    "            \n",
    "            except Exception as e:\n",
    "                if \"Degenerate labels\" in str(e) or \"Only one class present\" in str(e):\n",
    "                    y_pred = np.full(y_te.shape, np.nan)\n",
    "                else: raise e\n",
    "\n",
    "            if not np.isnan(y_pred).any():\n",
    "                results.append({'dataset': name, 'method': method,\n",
    "                                'accuracy': np.mean(y_pred == y_te.values),\n",
    "                                'spd': compute_spd(y_pred, s_te),\n",
    "                                'eod': compute_eod(y_te, y_pred, s_te)})\n",
    "\n",
    "# ─── Summarize results ─────────────────────────────────────────────────────────\n",
    "df_res = pd.DataFrame(results)\n",
    "if not df_res.empty:\n",
    "    agg = df_res.groupby(['dataset', 'method']).agg(\n",
    "        acc_mean=('accuracy', 'mean'), acc_std=('accuracy', 'std'),\n",
    "        spd_mean=('spd', 'mean'), spd_std=('spd', 'std'),\n",
    "        eod_mean=('eod', 'mean'), eod_std=('eod', 'std')).reset_index()\n",
    "    \n",
    "    def ci95(x):\n",
    "        if len(x) < 2: return f\"{x.mean():.3f} ± nan\"\n",
    "        mean, std_err = x.mean(), stats.sem(x)\n",
    "        h = std_err * stats.t.ppf((1 + 0.95) / 2., len(x)-1)\n",
    "        return f\"{mean:.3f} ± {h:.3f}\"\n",
    "\n",
    "    ci_results = df_res.groupby(['dataset', 'method']).apply(\n",
    "        lambda g: pd.Series({\n",
    "            'accuracy_CI95': ci95(g['accuracy']),\n",
    "            'spd_CI95': ci95(g['spd']),\n",
    "            'eod_CI95': ci95(g['eod'])\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    agg = pd.merge(agg, ci_results, on=['dataset', 'method'])\n",
    "\n",
    "    tests = []\n",
    "    for d in datasets:\n",
    "        if 'baseline' in df_res[df_res.dataset == d]['method'].unique():\n",
    "            base = df_res[(df_res.dataset == d) & (df_res.method == 'baseline')]['eod']\n",
    "            for m in ['pre', 'in', 'post']:\n",
    "                if m in df_res[df_res.dataset == d]['method'].unique():\n",
    "                    oth = df_res[(df_res.dataset == d) & (df_res.method == m)]['eod']\n",
    "                    if len(base) == len(oth) and len(base) > 1:\n",
    "                        t_stat, p_val = stats.ttest_rel(base, oth, nan_policy='omit')\n",
    "                        tests.append({'dataset': d, 'method': m, 't_stat': t_stat, 'p_val': p_val})\n",
    "    df_tests = pd.DataFrame(tests)\n",
    "\n",
    "    print(\"\\n=== CV Summary ===\")\n",
    "    print(agg.to_string(index=False))\n",
    "    print(\"\\n=== Paired T‑Test on EOD ===\")\n",
    "    print(df_tests.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results were generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
